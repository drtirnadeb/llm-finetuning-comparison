{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kcjWa4FPHoI"
   },
   "source": [
    "### Set up Google Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WO3hch3rPNim"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0OKDIlQPQSp"
   },
   "source": [
    "###  Load Pretrained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268,
     "referenced_widgets": [
      "37c62d35ebca406680730c6c4c604ab3",
      "652f793380f84a61ad1ef526f974baa5",
      "f97a747292034d2f8079c3d36d8790a1",
      "0383e24753e440c68c9744efc096a228",
      "c0b232a804244119b243ed9de0e8b22f",
      "012c13236a6748feb6332190e8c03f11",
      "a23055b545b64a0aa708511a79bf60ef",
      "dff19f939fbb40959939e0352bcd21ef",
      "ba42a16359024f49b6cc94d5ab19fe3e",
      "bfc66393786041cdb53e63e06a4e6bcc",
      "b849f74bcc644136a457fb58cb259181",
      "f0b237dbd02143a9ac80648ce620cd28",
      "dc9c089421144daca4090a5726704c7e",
      "eb33c4cb51004c54b3735d57b55bfcbb",
      "991bfe2ede7445dd9cd8fbf1cd28ee91",
      "fd87d9a12c8f4693a5dad313f654c552",
      "33e0559319d54a308b09cea827c9a545",
      "8e85d0d97d1049709192b67f35dd4370",
      "fbd2a0ab168e4991af9d3b36b4e190e4",
      "3143606803274d6ab79c0e8d08670dbe",
      "55225b35a6814d71bf48f78cb7835158",
      "0f82da64b8634ec790dce0e347a0e55b",
      "b4e38b75bff64808a1b28d37c63428aa",
      "0223f69ca6304fb1a4f34810c88d9332",
      "bad1b6dced714ae1adb1973b747bcbde",
      "24e7b872741d4bc2b4e039a7a3d982fb",
      "27ad6a7bc949442cbdf988a061fb1a33",
      "8b6579bd19b8412690754c491dca8016",
      "c91241a89e294b808318ee7a880abc10",
      "09e440b032e74cc5bba1e5b6ba8ca543",
      "be33b432d542471ba58c65f49ba07202",
      "988006772c34494d909b735ee7966b75",
      "df336c2e9e5841b7a3f55a04d6999ed3",
      "2b7cf6f0994a44da8a56dfee88874aee",
      "8ddaca74e9b7406897c31b31d35d7436",
      "61f7ca5088c446fca75466edb3378b58",
      "3050d5be3f2b4600ada5605a8eb84768",
      "6de9a9a006f5416c883cc4d7a6526762",
      "6504b4bdf17c4deda55dd3b9faceafa1",
      "0ae695a634984372b05cd400f72b8e3c",
      "36ef21d9181340c78607d60079afa14b",
      "223a79173de1403dac2ed6388eeb6ff6",
      "ef8081e9588d438da70ce4081718946c",
      "6b825597a3404f559e41424523ad29a1",
      "7079f49e93744774aa0285b20934bcb0",
      "9a626f23ca124d09b8fe463c71507a7f",
      "2dd0a14288ca475c971659c0b86c4c51",
      "0f0300dd3b6346f48068895be1e55be9",
      "ce904c02f847486998c986f23a047c3c",
      "c169d27c04534a0ca02f6f45884d3ca8",
      "d93932ed7f344947a325cf89ab92b0e9",
      "810d3c8c0190441abf86eabdbfd41787",
      "6c395f4a2ffd402e83c4c039da4a6e92",
      "7d538af733d249069a9281877512c27f",
      "85a6f85c9f4349febcfc743cfe7bc944"
     ]
    },
    "id": "phH7I_kyJOKr",
    "outputId": "bb07bb59-04d6-488a-f7ba-de552365f031"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#loading a model class that is already built on top of PyTorch (torch.nn.Module).\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) #because it is a binary classification\n",
    "#PyTorch model under the hood, so can use any PyTorch functions on it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ib60rp9iPY7D"
   },
   "source": [
    "### Freeze All Layers Except the Classifier\n",
    "\n",
    "Key Features of torch.nn.Module:\n",
    "\n",
    "It stores model layers\n",
    "\n",
    "It defines a forward() method (how data flows through the model)\n",
    "\n",
    "It keeps track of parameters (weights & biases) with model.parameters() or model.named_parameters()\n",
    "\n",
    "It handles things like saving, loading, GPU/CPU switching, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqS28UIaJOZ_",
    "outputId": "81ccdf49-fb57-4f2b-c625-fe14b9bf8539"
   },
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Print to verify\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM9FbFHyPbQT"
   },
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0vPUEwsJ1GF",
    "outputId": "26f317f5-3c68-4639-88d6-3f2010ab1831"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets fsspec\n",
    "\n",
    "# datasets: The Hugging Face library for loading and processing datasets.\n",
    "\n",
    "# fsspec: A file system interface used internally to load datasets from URLs, ZIPs, cloud, etc.\n",
    "\n",
    "#This ensures we are using the latest versions to avoid compatibility bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523,
     "referenced_widgets": [
      "8835a01fd83f4d18bf7de3892b4e030b",
      "f08cf03ca8af4865a8dceda44f72f1b5",
      "44e3ee97c9af4861b295c091c73198fe",
      "0d8a9795784f4a868de78f37c2028df5",
      "7d593c61814e498688e1c4515d72f3fa",
      "a958e8eac62d402fbc252b50477df3a0",
      "0ffdca2253544a7e8c6c6aecdbf7522c",
      "f6b3706025424475ab2e3e20830e68cb",
      "1fab255322f94885b7293373b53eaef3",
      "78e25472f60541268323c0ff284eed19",
      "4f548a2958d34e8da59a8ef03d0164f3",
      "b26e6aa08b94417984f4a4ef9e29dbc3",
      "857ce0102fe543b889e66a7b8a2999f0",
      "7364b43d14d44e4d91a2063242ed5eeb",
      "7867872dcc534230b941d16f5392658c",
      "19eb1b35179b4eb48bd860d4728256d2",
      "c824b0579ebd44519ec05401874a505d",
      "959b84cb1c494a488130f94c8d21c877",
      "2da9dfa9799b43e79daf6ba3406984ee",
      "32c0399284d04bce9294389b5ef19339",
      "4c3b7f47be5a44e2b394e0bdf21a9719",
      "32e36cfa195d4d4aaaad4cc97e73f408",
      "36b15b7320f44dfdbea318df06232c1c",
      "a0363d48d4904dc6a9a16f458e2a41d1",
      "401164f660844887b7b9541edb5b3f26",
      "9e3f309c74ac43a29d11c352cb4c32fb",
      "375d4c0bf66c4205a0450c596fd29f53",
      "f75bfd453b5944e2a2dd4343d2cc79d1",
      "9dce1f4b9a3041d19ea2052c2cdac897",
      "322dfbe7cb4a448b8d434c4d01ae1c25",
      "d70da1385653442e979af2b7ac8db205",
      "159a662373904ebc8029ac1fcd19cb6e",
      "5e49220b950440b48f54a009001ddb98",
      "7782ae8701bc44c9a5c13e659f44319c",
      "dfd6519220ca421e8672e7ea6b977ae8",
      "f0532d3b4fd5497499294e1ef22f49f5",
      "3eaec0b93fb3463cbf73cc1dce154735",
      "cf298b7ee730408e91e4d75c7590cc73",
      "0ebb87745d4b4ce9b8bb52b8a59a6f5f",
      "961b1cc4795d4861bbde251c702ed2d4",
      "cd7c0938e01743a483a63ea3deec03c9",
      "027fb2664f1542caae75ee9bd15a0723",
      "bc622a5ba228482d84e2da1dc347a722",
      "75ab1e5f175e41c69fa6d588d66741ba",
      "0a6b1adcb60d4e699c398af8cfd62351",
      "a17164fa266d4ad4aca517bfe6c4d44a",
      "5b2cecfa886b49e6ae5c3d0112b029b0",
      "512a28d9f55645dd9d29f7d8274ea359",
      "79711297acde4988b70db999520ba612",
      "8f48001ba6e54f7489fe9bce5ed0a623",
      "77c0489f526e46ac83d0f9651d898cf9",
      "c12f2543e3534636841e59ab3a9ddcf7",
      "5ed4081113604306a81d53217f30c975",
      "43e6d1f5ec4a45c7a59067865aef626b",
      "a2ad4510488f4b51839390462a7e8010"
     ]
    },
    "id": "RRpu5v7GJ1Je",
    "outputId": "7f90c05d-f103-45cb-cd90-9b654bd37a1b"
   },
   "outputs": [],
   "source": [
    "# Clear the datasets cache to force a fresh download\n",
    "!rm -rf ~/.cache/huggingface/datasets/*\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_polarity\") #This downloads the Yelp Polarity dataset.\n",
    "\n",
    "# It is a sentiment classification dataset: 2 labels: 1 = Positive, 0 = Negative\n",
    "# Texts = Yelp reviews\n",
    "\n",
    "\n",
    "small_train = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "small_test = dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# shuffle(seed=42): Randomizes the data (but reproducibly using the same seed).\n",
    "# select(range(...)): Selects a small subset for faster training/testing in Colab.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"][:10])  # First 5 samples\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lndaS9qVPq6P"
   },
   "source": [
    "###  Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "3b24914b89d14de0a5571e5460833d49",
      "76d435cc447c408cb313210585e3f5f5",
      "302e000804c54a1b864b1b79347f451d",
      "d20f616558dc411ebed1b771a05c8c5b",
      "8756f97a313a420599c15334312e838f",
      "5102a577c9a844c6a451b00df5dc9f17",
      "19931a27884e49dea4704f3f77a43765",
      "967ac3918a084eee83fbce467a70191e",
      "efea420b0b114457b62399efd7553de8",
      "ef7cb10752004200821db07824e2bfdb",
      "fb6ff317d8c74d1ca18551bbc74db1ca",
      "dd2ed424a67e4d8b8dd4bbd68c62f1e8",
      "baed6fcbc0c4469bb6e2081e26ac445f",
      "1849e840dbbf494c8f5e2f3f600306aa",
      "87c33c0ae0d24adaba24b5b52ec25aee",
      "e6542713fe894dd0b18c406e552fcc09",
      "7aa2fa125f1c4f8da547ff00e6792dcf",
      "542a7584f87644c1a58bea26ceb429af",
      "92cbace48985476aa033843461340aff",
      "6ef86087747c45269e1272603aa2023f",
      "cdf846968b8442919e5be87988951426",
      "6c258d813087432d9977e6f81d5b9b3a"
     ]
    },
    "id": "FB2M9V8SJ1NX",
    "outputId": "9cc3f6c2-b583-4f78-a728-2354d7eaaefe"
   },
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "#This takes a batch of text reviews (from examples[\"text\"]) and converts them into tokenized input using the model‚Äôs tokenizer.\n",
    "#Converts words into input IDs (numbers the model understands)\n",
    "#Adds special tokens (like [CLS], [SEP])\n",
    "#Applies truncation (cuts off long reviews)\n",
    "#Adds padding (makes all sequences the same length)\n",
    "\n",
    "encoded_train = small_train.map(preprocess, batched=True)\n",
    "encoded_test = small_test.map(preprocess, batched=True)\n",
    "\n",
    "#map(...) applies the preprocess() function to the entire dataset.\n",
    "#batched=True tells it to process in batches for speed (instead of one-by-one).\n",
    "\n",
    "#encoded_train and encoded_test contain:\n",
    "\n",
    "#input_ids: tokenized version of each review\n",
    "#attention_mask: tells the model which tokens are real vs. padding\n",
    "#label: same as before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvFEptZMPusL"
   },
   "source": [
    "### Fine-tune Only Last Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xT40BmBwKLzV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "#Do not use Weights & Biases (W&B) logging for this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 968,
     "referenced_widgets": [
      "06c8c5e301b9427995c27bd35f525d28",
      "9e20bbb5a13940dca5b0c2c284b9e9fb",
      "1c77678193d24e448b7ce67848c009a2",
      "bd52e916d8b74015905d5055782001b0",
      "7585e1b59cc4436196bcfaac934d52c9",
      "4d5311dce1e6472d855c2c34133a64d1",
      "d3eb5d82de604d8799b3e4c2d1b5d142",
      "c45a687a16594a97a9854f2fa94719b2",
      "02ed7775170a40099a2572663ff3344c",
      "0b8bf2b3633846c19a33a2d04c53a245",
      "51b0f0387ea54e14983230493f5d0083",
      "5e4af86529b445119906f26d60412d84",
      "7330db57b2da4fe880f7fe253bbb14ee",
      "40e0e236184b4f9288e96a11513074b1",
      "c6eca977067d47a38d8d2ae2fbedc0f7",
      "ef7e3b3d94324053b822cdf623655635",
      "df9904092789480faad30051222591c5",
      "1b60471504e143edb0250f9de7f06194",
      "ec70410a93a243ca9d6fa02b120e083f",
      "118b22cbda7542cab62299698b4d0a18",
      "45007e0a721649c69c1d614f17a36bf9",
      "4bf2f98e3aac482d814b28b7cc6d82e1"
     ]
    },
    "id": "kuMHaAhsKL2t",
    "outputId": "cd90688d-eded-45cc-9c01-6f62f5f8f221"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "!pip install evaluate # Explicitly install the evaluate library\n",
    "import evaluate\n",
    "\n",
    "#Trainer: high-level class to handle training, evaluation, logging, etc.\n",
    "#evaluate: library used to calculate metrics like accuracy, F1, etc.\n",
    "\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)  # Get predicted class\n",
    "    acc = accuracy.compute(predictions=preds, references=p.label_ids) # Compare to true labels\n",
    "    f1_score = f1.compute(predictions=preds, references=p.label_ids, average=\"weighted\") #average=\"weighted\" is useful when class balance isn't equal ‚Äî it weighs F1 by class size.\n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "        \"f1\": f1_score[\"f1\"]\n",
    "    }\n",
    "\n",
    "#This tells the trainer how to calculate accuracy after each evaluation.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", # Where to save model files\n",
    "    per_device_train_batch_size=8, # Batch size for training\n",
    "    per_device_eval_batch_size=8, # Batch size for evaluation\n",
    "    num_train_epochs=3, # Train for 3 full passes over the data\n",
    "    # Changed evaluation_strategy to eval_strategy\n",
    "    eval_strategy=\"epoch\", # Evaluate after every epoch\n",
    "    logging_steps=10, # Log training loss every 10 steps\n",
    "    save_strategy=\"no\" # Don‚Äôt save checkpoints\n",
    ")\n",
    "# telling the Trainer how to run the training loop.\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # The model I am fine-tuning\n",
    "    args=training_args, # The training configuration\n",
    "    train_dataset=encoded_train, # Training data\n",
    "    eval_dataset=encoded_test, # Evaluation data\n",
    "    compute_metrics=compute_metrics, # Function to calculate accuracy\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NE6-k6-P159"
   },
   "source": [
    "### Load New Model & Unfreeze Last Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBSTlLG0JOiV",
    "outputId": "c43cc84c-62e4-4433-b5b3-e4f875da52c4"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load a fresh model\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model2.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only last encoder layer\n",
    "for name, param in model2.named_parameters():\n",
    "    if \"transformer.layer.5\" in name:\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dou2Qen9P7GT"
   },
   "source": [
    "### Set New TrainingArguments and Train Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "Pqz7AysuKbP-",
    "outputId": "fa95d5b7-b124-4e32-d7c4-65da3b30d18f"
   },
   "outputs": [],
   "source": [
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"./model_last_layer_unfrozen\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs2\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2.train()\n",
    "\n",
    "# Save model and tokenizer\n",
    "model2.save_pretrained(\"./model_last_layer_unfrozen\")\n",
    "tokenizer.save_pretrained(\"./model_last_layer_unfrozen\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kenrxFJJP_m6"
   },
   "source": [
    "### Compare Both Training Runs (Plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "id": "Abu68KUpKbYF",
    "outputId": "69c41c1b-35d7-4891-ed2b-46f820cfeda6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use previous trainer (trainer1) and new trainer2\n",
    "history1 = trainer.state.log_history\n",
    "history2 = trainer2.state.log_history\n",
    "\n",
    "# Helper to extract accuracy and loss\n",
    "def extract_metrics(history):\n",
    "    epochs, accs, losses = [], [], []\n",
    "    for entry in history:\n",
    "        if 'eval_accuracy' in entry:\n",
    "            epochs.append(entry['epoch'])\n",
    "            accs.append(entry['eval_accuracy'])\n",
    "            losses.append(entry['eval_loss'])\n",
    "    return epochs, accs, losses\n",
    "\n",
    "epochs1, accs1, losses1 = extract_metrics(history1)\n",
    "epochs2, accs2, losses2 = extract_metrics(history2)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs1, accs1, label='Classifier Only', marker='o')\n",
    "plt.plot(epochs2, accs2, label='+Last Encoder Layer', marker='o')\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs1, losses1, label='Classifier Only', marker='o')\n",
    "plt.plot(epochs2, losses2, label='+Last Encoder Layer', marker='o')\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Extract F1 scores from training logs\n",
    "def extract_f1(history):\n",
    "    epochs, f1s = [], []\n",
    "    for entry in history:\n",
    "        if 'eval_f1' in entry:\n",
    "            epochs.append(entry['epoch'])\n",
    "            f1s.append(entry['eval_f1'])\n",
    "    return epochs, f1s\n",
    "\n",
    "f1_epochs1, f1s1 = extract_f1(history1)\n",
    "f1_epochs2, f1s2 = extract_f1(history2)\n",
    "\n",
    "# Plot F1 Score Comparison\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(f1_epochs1, f1s1, label='Classifier Only', marker='o')\n",
    "plt.plot(f1_epochs2, f1s2, label='+Last Encoder Layer', marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Validation F1 Score Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaUfQ4pyQIZc"
   },
   "source": [
    "### Observations\n",
    "\n",
    "In this project, I fine-tuned a DistilBERT model on a sentiment classification task using two strategies:\n",
    "\n",
    "(1) training only the final classification layer, and\n",
    "\n",
    "(2) additionally unfreezing the last encoder layer.\n",
    "\n",
    "The goal was to evaluate whether partial fine-tuning of the base model improves performance.\n",
    "\n",
    "\n",
    "From the validation accuracy and loss plots, it is clear that Model 2 (with the last encoder layer unfrozen) consistently outperforms Model 1. It achieves higher validation accuracy (~92% vs. ~88%) and maintains lower, more stable loss across epochs. These improvements suggest that even limited unfreezing allows the model to better adapt to the downstream task.\n",
    "\n",
    "The F1 score plot reinforces this conclusion ‚Äî Model 2 shows consistently higher F1 scores across epochs (~92% vs. ~87%), indicating stronger balance between precision and recall. This means Model 2 not only predicts more accurately, but also more reliably across both classes, making it more robust for real-world applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edu6t_5bOEAM"
   },
   "source": [
    "### üîç Prediction Function: `predict_review()`\n",
    "\n",
    "This function allows us to pass any text review into a fine-tuned DistilBERT model and receive a sentiment prediction along with a confidence score. It handles tokenization, device compatibility (CPU/GPU), and post-processing of the model output (logits to probabilities). The output includes the predicted label ‚Äî either \"Positive\" or \"Negative\" ‚Äî and a confidence score that reflects how sure the model is about its prediction. This function is especially useful for testing the model on custom inputs and comparing how different fine-tuning strategies affect prediction confidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGsFa3eUKbcx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_review(text, model, tokenizer):\n",
    "    # Tokenize the input text and return tensors in PyTorch format\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    #  Move the input tensors to the same device as the model (CPU or GPU)\n",
    "    device = model.device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    #  Run the model to get predictions\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    #  Convert raw logits to probabilities using softmax\n",
    "    probs = outputs.logits.softmax(dim=1)\n",
    "\n",
    "    #  Get the predicted class (0 or 1)\n",
    "    pred = probs.argmax().item()\n",
    "\n",
    "    #  Convert numerical label to human-readable text\n",
    "    label = \"Positive\" if pred == 1 else \"Negative\"\n",
    "\n",
    "    #  Get confidence score for the predicted label\n",
    "    confidence = probs[0][pred].item()\n",
    "\n",
    "    #  Return result as a dictionary\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"confidence\": round(confidence, 3)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH1b6qyGOS71"
   },
   "source": [
    "### üìä Model Agreement and Performance Evaluation\n",
    "\n",
    "In this section, we evaluate and compare the performance of the two fine-tuned models across the entire test set. We use the `predict_review()` function to generate predictions for all 500 test samples, and then compute how often the models agree on their predicted labels. In addition to agreement rate, we calculate each model's accuracy and F1 score using the true labels from the dataset. This gives us a quantitative understanding of how well each model performs overall, and whether unfreezing the last encoder layer leads to meaningful improvements in predictive accuracy and balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91hhZIo9JOol",
    "outputId": "9df0feb8-ddfb-44a8-ec33-cad3263d0572"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Step 1: Get model predictions\n",
    "def get_predictions(model, texts):\n",
    "    preds = []\n",
    "    for text in texts:\n",
    "        result = predict_review(text, model, tokenizer)\n",
    "        preds.append(result[\"label\"])\n",
    "    return preds\n",
    "\n",
    "# Step 2: Collect predictions on all test samples\n",
    "texts = small_test[\"text\"]\n",
    "true_labels = [\"Positive\" if label == 1 else \"Negative\" for label in small_test[\"label\"]]\n",
    "\n",
    "preds1 = get_predictions(model, texts)\n",
    "preds2 = get_predictions(model2, texts)\n",
    "\n",
    "# Step 3: Compare agreement\n",
    "agree_count = sum(p1 == p2 for p1, p2 in zip(preds1, preds2))\n",
    "total = len(texts)\n",
    "print(f\"Models agreed on {agree_count}/{total} reviews ({agree_count / total * 100:.2f}%)\")\n",
    "\n",
    "# Step 4: Compute individual performance\n",
    "# Agreement summary\n",
    "print(f\"Models agreed on {agree_count}/{total} reviews ({agree_count / total * 100:.2f}%)\")\n",
    "\n",
    "# Model 1 performance\n",
    "acc1 = accuracy_score(true_labels, preds1)\n",
    "f1_1 = f1_score(true_labels, preds1, pos_label=\"Positive\")\n",
    "\n",
    "# Model 2 performance\n",
    "acc2 = accuracy_score(true_labels, preds2)\n",
    "f1_2 = f1_score(true_labels, preds2, pos_label=\"Positive\")\n",
    "\n",
    "print(\"\\nüìä Model 1 (Classifier Only):\")\n",
    "print(f\"Accuracy: {round(acc1, 2)}\")\n",
    "print(f\"F1 Score: {round(f1_1, 2)}\")\n",
    "\n",
    "print(\"\\nüìä Model 2 (+Last Encoder Layer):\")\n",
    "print(f\"Accuracy: {round(acc2, 2)}\")\n",
    "print(f\"F1 Score: {round(f1_2, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqYaEiSeJOrv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
